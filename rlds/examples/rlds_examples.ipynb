{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gyFPhPWJev9"
      },
      "source": [
        "##### Copyright 2021 Google LLC. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPGlYwKdJP3o"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGMkkZI9gGVD"
      },
      "source": [
        "#**RLDS: Examples**\n",
        "This colab provides some examples of RLDS usage based on real use cases. If you are looking for an introduction to RLDS, see the [RLDS tutorial](https://colab.research.google.com/github/google-research/rlds/blob/main/rlds/examples/rlds_tutorial.ipynb) in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB0cAjdfrPXM"
      },
      "source": [
        "\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://colab.research.google.com/github/google-research/rlds/blob/main/rlds/examples/rlds_examples.ipynb\" target=\"_parent\"\u003e\u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Run In Google Colab\"/\u003e\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36WPcDI8lVPI"
      },
      "source": [
        "#Install Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1630078222348,
          "user": {
            "displayName": "",
            "photoUrl": "",
            "userId": ""
          },
          "user_tz": -180
        },
        "id": "LvD0ZsudlZVO"
      },
      "outputs": [],
      "source": [
        "!pip install rlds[tensorflow]\n",
        "!pip install tfds-nightly --upgrade\n",
        "!pip install envlogger\n",
        "!apt-get install libgmp-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tErv4WRmgTjE"
      },
      "source": [
        "##Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysYC-fdKjO3r"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import rlds\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwFBSgGjO3s"
      },
      "source": [
        "#Load dataset\n",
        "We can load the human dataset from the Panda Pick Place Can task of the [Robosuite collection in TFDS](https://www.tensorflow.org/datasets/catalog/overview#rlds). In these examples, we are assuming that certain fields are present in the steps, so datasets from different tasks will not be compatible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUPVer19cCqG"
      },
      "outputs": [],
      "source": [
        "dataset_config = 'human_dc29b40a' # @param { isTemplate : true}\n",
        "dataset_name = f'robosuite_panda_pick_place_can/{dataset_config}'\n",
        "num_episodes_to_load = 30 # @param { isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYFW_3X52y-h"
      },
      "source": [
        "# Learning from Demonstrations or Offline RL\n",
        "\n",
        "We consider the setup where an agent needs to solve a task specified by a reward $r$. We assume a dataset of episodes with the corresponding rewards is available for training. This includes:\n",
        "*  The ORL setup [[1], [2] [3]] where the agent is trained solely from a dataset of episodes collected in the environment.\n",
        "* The LfD setup [[4], [5], [6], [7]] where the agent can also interact with the environment.\n",
        "\n",
        "Using one of the two provided datasets on the Robosuite PickPlaceCan environment, a typical RLDS pipeline would include the following steps:\n",
        "\n",
        "1. sample $K$ episodes from the dataset so the performance of the trained agent could be expressed as a function of the number of available episodes.\n",
        "1. combine the observations used as an input of the agent. The Robosuite datasets include many fields in the observations and one could try to train the agent from the state or form the visual observations for example.\n",
        "1. finally, convert the dataset of episodes into a dataset of transitions that can be consumed by algorithms such as SAC or TD3.\n",
        "\n",
        "[1]:(https://arxiv.org/abs/2005.01643)\n",
        "[2]:(https://arxiv.org/abs/1911.11361)\n",
        "[3]:(https://arxiv.org/abs/2103.01948)\n",
        "[4]:(https://arxiv.org/abs/1909.01387)\n",
        "[5]:(https://arxiv.org/abs/1704.03732)\n",
        "[6]:(https://arxiv.org/abs/1707.08817)\n",
        "[7]:(https://arxiv.org/abs/2006.12917)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAnUJzN03eij"
      },
      "outputs": [],
      "source": [
        "K = 5 # @param { isTemplate: true}\n",
        "buffer_size = 30 # @param { isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pol7u-_D3Bwo"
      },
      "outputs": [],
      "source": [
        "dataset = tfds.load(dataset_name, split=f'train[:{num_episodes_to_load}]')\n",
        "dataset = dataset.shuffle(buffer_size, seed=42, reshuffle_each_iteration=False)\n",
        "dataset = dataset.take(K)\n",
        "\n",
        "def prepare_observation(step):\n",
        "  \"\"\"Filters the obseravtion to only keep the state and flattens it.\"\"\"\n",
        "  observation_names = ['robot0_proprio-state', 'object-state']\n",
        "  step[rlds.OBSERVATION] = tf.concat(\n",
        "      [step[rlds.OBSERVATION][key] for key in observation_names], axis=-1)\n",
        "  return step\n",
        "dataset = rlds.transformations.map_nested_steps(dataset, prepare_observation)\n",
        "\n",
        "def batch_to_transition(batch):\n",
        "  \"\"\"Converts a pair of consecutive steps to a custom transition format.\"\"\"\n",
        "  return {'s_cur': batch[rlds.OBSERVATION][0],\n",
        "          'a': batch[rlds.ACTION][0],\n",
        "          'r': batch[rlds.REWARD][0],\n",
        "          's_next': batch[rlds.OBSERVATION][1]}\n",
        "\n",
        "def make_transition_dataset(episode):\n",
        "  \"\"\"Converts an episode of steps to a dataset of custom transitions.\"\"\"\n",
        "  # Create a dataset of 2-step sequences with overlap of 1.\n",
        "  batched_steps = rlds.transformations.batch(episode[rlds.STEPS], size=2, shift=1)\n",
        "  return batched_steps.map(batch_to_transition)\n",
        "\n",
        "transitions_ds = dataset.flat_map(make_transition_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWNhxwJzOUJv"
      },
      "source": [
        "# Absorbing Terminal States in Imitation Learning\n",
        "\n",
        "Imitation learning is the setup where an agent tries to imitate a behavior, as defined by some sample episodes of that behavior.\n",
        "In particular, the reward is not specified.\n",
        "\n",
        "The dataset processing pipeline requires all the different pieces seen in the learning from demonstrations setup (create a train split, assemble the observation, ...) but also has some specifics.\n",
        "One specific is related to the particular role of the terminal state in imitation learning.\n",
        "While in standard RL tasks, looping over the terminal states only brings zero in terms of reward, in imitation learning, making this assumption of zero reward for transitions from a terminal state to the same terminal state induces some bias in algorithms like GAIL.\n",
        "One way to counter this bias was proposed in [1]. It consists in learning the reward value of the transition from the absorbing state to itself.\n",
        "Implementation wise, to tell a terminal state from another state, an `absorbing` bit is added to the observation (`1` for a terminal state, `0` for a regular state). The dataset is also augmented with terminal state to terminal state transitions so the agent can learn from those transitions.\n",
        "\n",
        "[1]:(https://arxiv.org/abs/1809.02925)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu8IW4u3PA1S"
      },
      "outputs": [],
      "source": [
        "def duplicate_terminal_step(episode):\n",
        "  \"\"\"Duplicates the terminal step if the episode ends in one. Noop otherwise.\"\"\"\n",
        "  return rlds.transformations.concat_if_terminal(\n",
        "      episode, make_extra_steps=tf.data.Dataset.from_tensors)\n",
        "\n",
        "def convert_to_absorbing_state(step):\n",
        "  padding = step[rlds.IS_TERMINAL]\n",
        "  if step[rlds.IS_TERMINAL]:\n",
        "    step[rlds.OBSERVATION] = tf.zeros_like(step[rlds.OBSERVATION])\n",
        "    step[rlds.ACTION] = tf.zeros_like(step[rlds.ACTION])\n",
        "    # This is no longer a terminal state as the episode loops indefinitely.\n",
        "    step[rlds.IS_TERMINAL] = False\n",
        "    step[rlds.IS_LAST] = False\n",
        "  # Add the absorbing bit to the observation.\n",
        "  step[rlds.OBSERVATION] = tf.concat([step[rlds.OBSERVATION], [padding]], 0)\n",
        "  return step\n",
        "\n",
        "absorbing_state_ds = rlds.transformations.apply_nested_steps(\n",
        "    dataset, duplicate_terminal_step)\n",
        "absorbing_state_ds = rlds.transformations.map_nested_steps(\n",
        "    absorbing_state_ds, convert_to_absorbing_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSmC3C1JPLyp"
      },
      "source": [
        "# Offline Analysis\n",
        "\n",
        "One significant use case we envision for RLDS is the offline analysis of collected datasets.\n",
        "There is no standard offline analysis procedure as what is possible is only limited by the imagination of the users. We expose in this section a fictitious use case to illustrate how custom tags stored in a RL dataset can be processed as part of an RLDS pipeline.\n",
        "Let's assume we want to generate an histogram of the returns of the episodes present in the provided dataset of human episodes on the robosuite PickPlaceCan environment. This dataset holds episodes of fixed length of size 400 but also has a tag to indicate the actual end of the task.\n",
        "We consider here the histogram of returns of the variable length episodes ending on the completion tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVol8u63PcEV"
      },
      "outputs": [],
      "source": [
        "def placed_tag_is_set(step):\n",
        "  return tf.not_equal(tf.math.count_nonzero(step['tag:placed']),0)\n",
        "\n",
        "def compute_return(steps):\n",
        "  \"\"\"Computes the return of the episode up to the 'placed' tag.\"\"\"\n",
        "  # Truncate the episode after the placed tag.\n",
        "  steps = rlds.transformations.truncate_after_condition(\n",
        "      steps, truncate_condition=placed_tag_is_set)\n",
        "  return rlds.transformations.sum_dataset(steps, lambda step: step[rlds.REWARD])\n",
        "\n",
        "returns_ds = dataset.map(lambda episode: compute_return(episode[rlds.STEPS]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "rlds_examples.ipynb",
      "provenance": [
        {
          "file_id": "1jD_bvd96oC2QgiplwE_SGj9rpNlQdGbm",
          "timestamp": 1630054663032
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
